{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796428d0-851a-4c46-87fd-4483f2b42cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f680962e-e3b2-4cc4-92d3-2c4e7ce7a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"C:/Users/yasin/Desktop/birlesik_yorumlar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d99a91e-e245-4e6d-beea-01ef6dbf3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The fries were terrific also, hot crisp...”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“I love the food and our server Maria!”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“The filet mignon was impeccable and the musse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“My seafood cocktail had wonderful large lump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“penne al pomodoro and bucatini cacio e pepe w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17393</th>\n",
       "      <td>The room was good, comfortable and aesthetic \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17394</th>\n",
       "      <td>good hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395</th>\n",
       "      <td>good experience for me about hotel \\nvery good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17396</th>\n",
       "      <td>well done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17397</th>\n",
       "      <td>Nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0           “The fries were terrific also, hot crisp...”\n",
       "1                “I love the food and our server Maria!”\n",
       "2      “The filet mignon was impeccable and the musse...\n",
       "3      “My seafood cocktail had wonderful large lump ...\n",
       "4      “penne al pomodoro and bucatini cacio e pepe w...\n",
       "...                                                  ...\n",
       "17393  The room was good, comfortable and aesthetic \\...\n",
       "17394                                         good hotel\n",
       "17395  good experience for me about hotel \\nvery good...\n",
       "17396                                          well done\n",
       "17397                                            Nothing\n",
       "\n",
       "[17398 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7a0a52-b444-4d5e-a13a-e40ab6d38eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varsayalım ki DataFrame'iniz df ve yorumlar '0' sütununda\n",
    "text = ' '.join(df['0'].astype(str).tolist())  # Tüm yorumları tek bir metin haline getir\n",
    "sentences = sent_tokenize(text)  # Metni cümlelere ayır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76765dbb-b1ff-4781-b70e-24d05601ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer ve Stemmer'ı başlat\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705265c3-35f5-4a67-a2a9-53a4e6f2c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords listesini almak\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d397b6f5-8b02-4561-8ded-6a1c1b0a2ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a9255c-6774-4f80-a60b-26f85f1d4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimeleri tokenleştirip, lemmatize etme ve stemleme\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)  # Cümleyi kelimelere ayır\n",
    "    # Sadece harf olan kelimeleri al ve stopword'leri çıkar\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize etme\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Stemleme\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be15474a-df93-472e-b383-60dd4a21991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her cümleyi tokenleştir, lemmatize et ve stemle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ee6eeb-48ee-4db4-a3f2-5e8126009bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a99c4f6-9939-4833-bcf5-43e67cda6ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fry',\n",
       "  'terrific',\n",
       "  'also',\n",
       "  'hot',\n",
       "  'crisp',\n",
       "  'love',\n",
       "  'food',\n",
       "  'server',\n",
       "  'maria',\n",
       "  'filet',\n",
       "  'mignon',\n",
       "  'impeccable',\n",
       "  'mussel',\n",
       "  'steak',\n",
       "  'tartare',\n",
       "  'also',\n",
       "  'v',\n",
       "  'seafood',\n",
       "  'cocktail',\n",
       "  'wonderful',\n",
       "  'large',\n",
       "  'lump',\n",
       "  'crabmeat',\n",
       "  'delicious',\n",
       "  'lobster',\n",
       "  'penne',\n",
       "  'al',\n",
       "  'pomodoro',\n",
       "  'bucatini',\n",
       "  'cacio',\n",
       "  'e',\n",
       "  'pepe',\n",
       "  'highly',\n",
       "  'recommend',\n",
       "  'onion',\n",
       "  'soup',\n",
       "  'mussel',\n",
       "  'french',\n",
       "  'great',\n",
       "  'food',\n",
       "  'good',\n",
       "  'price',\n",
       "  'best',\n",
       "  'cocktail',\n",
       "  'worth',\n",
       "  'travelling',\n",
       "  'mile',\n",
       "  'incredible',\n",
       "  'italian',\n",
       "  'perfect',\n",
       "  'pizza',\n",
       "  'top',\n",
       "  'splendid',\n",
       "  'pizza',\n",
       "  'excellent',\n",
       "  'french',\n",
       "  'onion',\n",
       "  'soup',\n",
       "  'food',\n",
       "  'great',\n",
       "  'especially',\n",
       "  'mussel',\n",
       "  'good',\n",
       "  'fresh'],\n",
       " ['abso',\n",
       "  'loved',\n",
       "  'charcuterie',\n",
       "  'board',\n",
       "  'burger',\n",
       "  'course',\n",
       "  'onion',\n",
       "  'soup',\n",
       "  'chicken',\n",
       "  'tikka',\n",
       "  'masala',\n",
       "  'favorite',\n",
       "  'good',\n",
       "  'ramen',\n",
       "  'frill',\n",
       "  'wing',\n",
       "  'crispy',\n",
       "  'duck',\n",
       "  'fried',\n",
       "  'rice',\n",
       "  'far',\n",
       "  'favorite',\n",
       "  'delicious',\n",
       "  'breakfast',\n",
       "  'sandwich',\n",
       "  'cou',\n",
       "  'buratta',\n",
       "  'arancini',\n",
       "  'bucatini',\n",
       "  'lasagna',\n",
       "  'everything',\n",
       "  'fish',\n",
       "  'fresh',\n",
       "  'moist',\n",
       "  'lamb',\n",
       "  'highly',\n",
       "  'recommend',\n",
       "  'branzino',\n",
       "  'ravioli',\n",
       "  'adult',\n",
       "  'devoured',\n",
       "  'ceviche',\n",
       "  'octopus',\n",
       "  'enjoying',\n",
       "  'g',\n",
       "  'every',\n",
       "  'bite'],\n",
       " ['authentic',\n",
       "  'delicious',\n",
       "  'korean',\n",
       "  'cuisine',\n",
       "  'example',\n",
       "  'sea',\n",
       "  'urchin',\n",
       "  'bibimbab',\n",
       "  'black',\n",
       "  'cod',\n",
       "  'clam',\n",
       "  'chicken',\n",
       "  'satay',\n",
       "  'bun',\n",
       "  'start',\n",
       "  'followed',\n",
       "  'prawn',\n",
       "  'pad',\n",
       "  'thai',\n",
       "  'fantastic',\n",
       "  'beef',\n",
       "  'excellent',\n",
       "  'food',\n",
       "  'others',\n",
       "  'bolognese',\n",
       "  'followed',\n",
       "  'pork',\n",
       "  'chop',\n",
       "  'soup',\n",
       "  'slightly',\n",
       "  'salty',\n",
       "  'pork',\n",
       "  'noodle',\n",
       "  'inside',\n",
       "  'great',\n",
       "  'quality',\n",
       "  'price',\n",
       "  'started',\n",
       "  'loaded',\n",
       "  'superb',\n",
       "  'steak',\n",
       "  'really',\n",
       "  'delicious',\n",
       "  'chicken',\n",
       "  'piccata',\n",
       "  'casual',\n",
       "  'dinner',\n",
       "  'best',\n",
       "  'brunch',\n",
       "  'fluffiest',\n",
       "  'french',\n",
       "  'toast',\n",
       "  'ribeye',\n",
       "  'steak',\n",
       "  'excellent',\n",
       "  'quality',\n",
       "  'meat',\n",
       "  'beautifully',\n",
       "  'cooked',\n",
       "  'ribeye',\n",
       "  'steak',\n",
       "  'excellent',\n",
       "  'quality',\n",
       "  'meat',\n",
       "  'beautifully',\n",
       "  'cooked',\n",
       "  'given',\n",
       "  'complimentary',\n",
       "  'pistachio',\n",
       "  'tiramisu',\n",
       "  'light',\n",
       "  'fluffy',\n",
       "  'crun',\n",
       "  'pleased',\n",
       "  'duck',\n",
       "  'pizza',\n",
       "  'kale',\n",
       "  'salad',\n",
       "  'rich',\n",
       "  'chocolate',\n",
       "  'p',\n",
       "  'taco',\n",
       "  'salad',\n",
       "  'nyc',\n",
       "  'pizzeria',\n",
       "  'amazing']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_lemmatized[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ccdde98-b9f7-4abd-be3a-1a1d7a59e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e67ec0-2dd4-4953-93d7-35b6804675e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fry terrific also hot crisp love food server maria filet mignon impeccable mussel steak tartare also v seafood cocktail wonderful large lump crabmeat delicious lobster penne al pomodoro bucatini cacio e pepe highly recommend onion soup mussel french great food good price best cocktail worth travelling mile incredible italian perfect pizza top splendid pizza excellent french onion soup food great especially mussel good fresh',\n",
       " 'abso loved charcuterie board burger course onion soup chicken tikka masala favorite good ramen frill wing crispy duck fried rice far favorite delicious breakfast sandwich cou buratta arancini bucatini lasagna everything fish fresh moist lamb highly recommend branzino ravioli adult devoured ceviche octopus enjoying g every bite',\n",
       " 'authentic delicious korean cuisine example sea urchin bibimbab black cod clam chicken satay bun start followed prawn pad thai fantastic beef excellent food others bolognese followed pork chop soup slightly salty pork noodle inside great quality price started loaded superb steak really delicious chicken piccata casual dinner best brunch fluffiest french toast ribeye steak excellent quality meat beautifully cooked ribeye steak excellent quality meat beautifully cooked given complimentary pistachio tiramisu light fluffy crun pleased duck pizza kale salad rich chocolate p taco salad nyc pizzeria amazing']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ön işlenmiş token listelerini tekrar metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "\n",
    "lemmatized_texts[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745ebc2f-d8ef-4d2b-9d78-dd3ddcf992d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aakash  aake  aall  aaminties  aap  aapse  aashram  aayush  abace  \\\n",
      "0     0.0   0.0   0.0        0.0  0.0    0.0      0.0     0.0    0.0   \n",
      "1     0.0   0.0   0.0        0.0  0.0    0.0      0.0     0.0    0.0   \n",
      "2     0.0   0.0   0.0        0.0  0.0    0.0      0.0     0.0    0.0   \n",
      "3     0.0   0.0   0.0        0.0  0.0    0.0      0.0     0.0    0.0   \n",
      "4     0.0   0.0   0.0        0.0  0.0    0.0      0.0     0.0    0.0   \n",
      "\n",
      "   abandoned  ...  カジュアル   外人  早餐好吃  熱水量又大又舒服  第一次  감자튀김  맛있고  바삭한  샐러드도  편하고  \n",
      "0        0.0  ...    0.0  0.0   0.0       0.0  0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "1        0.0  ...    0.0  0.0   0.0       0.0  0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "2        0.0  ...    0.0  0.0   0.0       0.0  0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "3        0.0  ...    0.0  0.0   0.0       0.0  0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "4        0.0  ...    0.0  0.0   0.0       0.0  0.0   0.0  0.0  0.0   0.0  0.0  \n",
      "\n",
      "[5 rows x 6596 columns]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vektörizerı başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "#terim frekansları, belge frekanslarıni hesplar\n",
    "#TF-IDF vektörlerine dönüştürür\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "\n",
    "## Kelimeleri alalım\n",
    "#F-IDF vektörleştirme işleminde kullanılan tüm kelimelerin essiz bir listesini döndürur\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF-IDF matrisini pandas DataFrame'e çevir-gorunurluk acisindan- calismasi kolay\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# İlk birkaç satırı gösterelim-ilk 5 cümle\n",
    "print(tfidf_df.head())\n",
    "\n",
    "#Her satır bir cümleyi temsil eder\n",
    "#Her sütun bir kelimeyi temsil eder\n",
    "#Hücreler ise o kelimenin o cümledeki TF-IDF skorudur - her cumle icin degisir-bakiniz:slaytlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "151a66a4-2ff5-40d6-b20e-8f5a94caa764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "mussel      0.407469\n",
      "onion       0.216783\n",
      "cocktail    0.212663\n",
      "soup        0.200253\n",
      "french      0.196796\n",
      "pizza       0.175335\n",
      "food        0.170931\n",
      "pomodoro    0.163968\n",
      "crabmeat    0.163968\n",
      "penne       0.163968\n",
      "lump        0.163968\n",
      "maria       0.163968\n",
      "bucatini    0.151847\n",
      "splendid    0.151847\n",
      "cacio       0.151847\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını al\n",
    "first_sentence_vector = tfidf_df.iloc[0]\n",
    "\n",
    "# Skorlara göre sırala (yüksekten düşüğe)\n",
    "top_5_words = first_sentence_vector.sort_values(ascending=False).head(15)\n",
    "\n",
    "# Sonucu yazdır\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d70aff-4abb-4991-b49f-ffd6f2b3f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "328fe184-d9f0-4f62-a337-65377f5f6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soup: 1.0000\n",
      "onion: 0.6992\n",
      "french: 0.6077\n",
      "tuna: 0.3618\n",
      "toast: 0.3406\n",
      "charles: 0.3390\n"
     ]
    }
   ],
   "source": [
    "# çorba kelimesinin vektörünü alalım\n",
    "soup_index = feature_names.tolist().index('soup')  # 'soup' kelimesinin indeksini bul\n",
    "\n",
    "# çorba kelimesinin TF-IDF vektörünü alıyoruz ve 2D formatta yapıyoruz\n",
    "soup_vector = tfidf_matrix[:, soup_index].toarray()\n",
    "\n",
    "# Tüm kelimelerin TF-IDF vektörlerini alıyoruz\n",
    "tfidf_vectors = tfidf_matrix.toarray()\n",
    "\n",
    "# Cosine similarity hesaplayalım\n",
    "similarities = cosine_similarity(soup_vector.T, tfidf_vectors.T)\n",
    "\n",
    "# Benzerlikleri sıralayalım ve en yüksek 5 kelimeyi seçelim\n",
    "similarities = similarities.flatten()\n",
    "top_5_indices = similarities.argsort()[-6:][::-1]  # 6. en büyükten başlıyoruz çünkü kendisi de dahil\n",
    "\n",
    "# Sonuçları yazdıralım\n",
    "for index in top_5_indices:\n",
    "    print(f\"{feature_names[index]}: {similarities[index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18b0ee-d964-4f43-a666-402a0e473a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
