{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c970e0c9-15ea-48ea-8821-21761fca7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82015034-8f76-48c6-8668-106c6eed62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"C:/Users/yasin/Desktop/birlesik_yorumlar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee83674-6461-4d0d-8693-6282189def55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“The fries were terrific also, hot crisp...”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“I love the food and our server Maria!”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“The filet mignon was impeccable and the musse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“My seafood cocktail had wonderful large lump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“penne al pomodoro and bucatini cacio e pepe w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17393</th>\n",
       "      <td>The room was good, comfortable and aesthetic \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17394</th>\n",
       "      <td>good hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17395</th>\n",
       "      <td>good experience for me about hotel \\nvery good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17396</th>\n",
       "      <td>well done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17397</th>\n",
       "      <td>Nothing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0           “The fries were terrific also, hot crisp...”\n",
       "1                “I love the food and our server Maria!”\n",
       "2      “The filet mignon was impeccable and the musse...\n",
       "3      “My seafood cocktail had wonderful large lump ...\n",
       "4      “penne al pomodoro and bucatini cacio e pepe w...\n",
       "...                                                  ...\n",
       "17393  The room was good, comfortable and aesthetic \\...\n",
       "17394                                         good hotel\n",
       "17395  good experience for me about hotel \\nvery good...\n",
       "17396                                          well done\n",
       "17397                                            Nothing\n",
       "\n",
       "[17398 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4968e3-ea3f-4976-9b6a-310900276b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varsayalım ki DataFrame'iniz df ve yorumlar '0' sütununda\n",
    "text = ' '.join(df['0'].astype(str).tolist())  # Tüm yorumları tek bir metin haline getir\n",
    "sentences = sent_tokenize(text)  # Metni cümlelere ayır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f625eb-3b6a-46bc-b1c5-644fb61cc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer ve Stemmer'ı başlat\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968cfc65-5f2c-4391-8ec8-207ee9ee0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords listesini almak\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ba609c6-009f-4401-91c4-e5a9e489f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimeleri tokenleştirip, lemmatize etme ve stemleme\n",
    "def preprocess_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)  # Cümleyi kelimelere ayır\n",
    "    # Sadece harf olan kelimeleri al ve stopword'leri çıkar\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatize etme\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]  # Stemleme\n",
    "    \n",
    "    return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7208ce67-63ab-487e-bdd9-e971437154b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her cümleyi tokenleştir, lemmatize et ve stemle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db23f42-b1b5-4b8f-b6b9-cabbbce9683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    "    tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3792cbfb-f133-4340-ad11-ef4a84640f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Lemmatized: ['fry', 'terrific', 'also', 'hot', 'crisp', 'love', 'food', 'server', 'maria', 'filet', 'mignon', 'impeccable', 'mussel', 'steak', 'tartare', 'also', 'v', 'seafood', 'cocktail', 'wonderful', 'large', 'lump', 'crabmeat', 'delicious', 'lobster', 'penne', 'al', 'pomodoro', 'bucatini', 'cacio', 'e', 'pepe', 'highly', 'recommend', 'onion', 'soup', 'mussel', 'french', 'great', 'food', 'good', 'price', 'best', 'cocktail', 'worth', 'travelling', 'mile', 'incredible', 'italian', 'perfect', 'pizza', 'top', 'splendid', 'pizza', 'excellent', 'french', 'onion', 'soup', 'food', 'great', 'especially', 'mussel', 'good', 'fresh']\n",
      "Cümle 1 - Stemmed: ['fri', 'terrif', 'also', 'hot', 'crisp', 'love', 'food', 'server', 'maria', 'filet', 'mignon', 'impecc', 'mussel', 'steak', 'tartar', 'also', 'v', 'seafood', 'cocktail', 'wonder', 'larg', 'lump', 'crabmeat', 'delici', 'lobster', 'penn', 'al', 'pomodoro', 'bucatini', 'cacio', 'e', 'pepe', 'highli', 'recommend', 'onion', 'soup', 'mussel', 'french', 'great', 'food', 'good', 'price', 'best', 'cocktail', 'worth', 'travel', 'mile', 'incred', 'italian', 'perfect', 'pizza', 'top', 'splendid', 'pizza', 'excel', 'french', 'onion', 'soup', 'food', 'great', 'especi', 'mussel', 'good', 'fresh']\n",
      "\n",
      "\n",
      "Cümle 2 - Lemmatized: ['abso', 'loved', 'charcuterie', 'board', 'burger', 'course', 'onion', 'soup', 'chicken', 'tikka', 'masala', 'favorite', 'good', 'ramen', 'frill', 'wing', 'crispy', 'duck', 'fried', 'rice', 'far', 'favorite', 'delicious', 'breakfast', 'sandwich', 'cou', 'buratta', 'arancini', 'bucatini', 'lasagna', 'everything', 'fish', 'fresh', 'moist', 'lamb', 'highly', 'recommend', 'branzino', 'ravioli', 'adult', 'devoured', 'ceviche', 'octopus', 'enjoying', 'g', 'every', 'bite']\n",
      "Cümle 2 - Stemmed: ['abso', 'love', 'charcuteri', 'board', 'burger', 'cours', 'onion', 'soup', 'chicken', 'tikka', 'masala', 'favorit', 'good', 'ramen', 'frill', 'wing', 'crispi', 'duck', 'fri', 'rice', 'far', 'favorit', 'delici', 'breakfast', 'sandwich', 'cou', 'buratta', 'arancini', 'bucatini', 'lasagna', 'everyth', 'fish', 'fresh', 'moist', 'lamb', 'highli', 'recommend', 'branzino', 'ravioli', 'adult', 'devour', 'cevich', 'octopu', 'enjoy', 'g', 'everi', 'bite']\n",
      "\n",
      "\n",
      "Cümle 3 - Lemmatized: ['authentic', 'delicious', 'korean', 'cuisine', 'example', 'sea', 'urchin', 'bibimbab', 'black', 'cod', 'clam', 'chicken', 'satay', 'bun', 'start', 'followed', 'prawn', 'pad', 'thai', 'fantastic', 'beef', 'excellent', 'food', 'others', 'bolognese', 'followed', 'pork', 'chop', 'soup', 'slightly', 'salty', 'pork', 'noodle', 'inside', 'great', 'quality', 'price', 'started', 'loaded', 'superb', 'steak', 'really', 'delicious', 'chicken', 'piccata', 'casual', 'dinner', 'best', 'brunch', 'fluffiest', 'french', 'toast', 'ribeye', 'steak', 'excellent', 'quality', 'meat', 'beautifully', 'cooked', 'ribeye', 'steak', 'excellent', 'quality', 'meat', 'beautifully', 'cooked', 'given', 'complimentary', 'pistachio', 'tiramisu', 'light', 'fluffy', 'crun', 'pleased', 'duck', 'pizza', 'kale', 'salad', 'rich', 'chocolate', 'p', 'taco', 'salad', 'nyc', 'pizzeria', 'amazing']\n",
      "Cümle 3 - Stemmed: ['authent', 'delici', 'korean', 'cuisin', 'exampl', 'sea', 'urchin', 'bibimbab', 'black', 'cod', 'clam', 'chicken', 'satay', 'bun', 'start', 'follow', 'prawn', 'pad', 'thai', 'fantast', 'beef', 'excel', 'food', 'other', 'bolognes', 'follow', 'pork', 'chop', 'soup', 'slightli', 'salti', 'pork', 'noodl', 'insid', 'great', 'qualiti', 'price', 'start', 'load', 'superb', 'steak', 'realli', 'delici', 'chicken', 'piccata', 'casual', 'dinner', 'best', 'brunch', 'fluffiest', 'french', 'toast', 'ribey', 'steak', 'excel', 'qualiti', 'meat', 'beauti', 'cook', 'ribey', 'steak', 'excel', 'qualiti', 'meat', 'beauti', 'cook', 'given', 'complimentari', 'pistachio', 'tiramisu', 'light', 'fluffi', 'crun', 'pleas', 'duck', 'pizza', 'kale', 'salad', 'rich', 'chocol', 'p', 'taco', 'salad', 'nyc', 'pizzeria', 'amaz']\n",
      "\n",
      "\n",
      "Cümle 4 - Lemmatized: ['best', 'steak', 'vibe', 'city', 'stand', 'dish', 'maine', 'lobster', 'salad', 'maine', 'best', 'risotto', 'ever', 'life', 'thus', 'loved', 'small', 'cozy', 'delicious', 'great', 'authentic', 'american', 'breakfast', 'kitchen', 'favorite', 'flatbread', 'steak', 'tartare', 'enjoyed', 'performance', 'hamilton', 'leithauser', 'back', 'sushi', 'fresh', 'delicious', 'fried', 'rice', 'also', 'food', 'also', 'tasty', 'get', 'duck', 'ramen', 'favorite', 'best', 'cocktail', 'sushi', 'ramen', 'particularly', 'various', 'dish', 'blistered', 'pepper', 'patatas', 'bravas', 'croquette', 'awesome', 'first', 'visit']\n",
      "Cümle 4 - Stemmed: ['best', 'steak', 'vibe', 'citi', 'stand', 'dish', 'main', 'lobster', 'salad', 'main', 'best', 'risotto', 'ever', 'live', 'thu', 'love', 'small', 'cozi', 'delici', 'great', 'authent', 'american', 'breakfast', 'kitchen', 'favorit', 'flatbread', 'steak', 'tartar', 'enjoy', 'perform', 'hamilton', 'leithaus', 'back', 'sushi', 'fresh', 'delici', 'fri', 'rice', 'also', 'food', 'also', 'tasti', 'get', 'duck', 'ramen', 'favorit', 'best', 'cocktail', 'sushi', 'ramen', 'particularli', 'variou', 'dish', 'blister', 'pepper', 'patata', 'brava', 'croquett', 'awesom', 'first', 'visit']\n",
      "\n",
      "\n",
      "Cümle 5 - Lemmatized: ['coq', 'au', 'vin', 'tasty', 'moist', 'chicken', 'served', 'delicate', 'sauce', 'enjoyed', 'dumpling', 'bulgogi', 'chicken', 'pork', 'belly', 'spare', 'great', 'food', 'good', 'price', 'filet', 'mignon', 'world', 'cooked', 'absolute', 'son', 'got', 'salmon', 'said', 'ordered', 'waited', 'bar', 'lunch', 'chelsea', 'nyc', 'especially', 'lobster', 'caviar', 'also', 'pork', 'ginger', 'come', 'salad', 'appetizer', 'delicious', 'dish', 'good', 'perfect', 'ny', 'bagel', 'asto', 'great', 'place', 'irish', 'breakfast', 'delicious', 'mexican', 'food', 'close', 'theatre', 'district', 'slider', 'green', 'salad', 'tasty', 'lemony', 'notch', 'including', 'octopus', 'tuna', 'tartare', 'serve', 'breakfast', 'day', 'including', 'egg', 'bagel', 'smoked', 'fish', 'matzoh', 'bal', 'tony', 'thank', 'god']\n",
      "Cümle 5 - Stemmed: ['coq', 'au', 'vin', 'tasti', 'moist', 'chicken', 'serv', 'delic', 'sauc', 'enjoy', 'dumpl', 'bulgogi', 'chicken', 'pork', 'belli', 'spare', 'great', 'food', 'good', 'price', 'filet', 'mignon', 'world', 'cook', 'absolut', 'son', 'got', 'salmon', 'said', 'order', 'wait', 'bar', 'lunch', 'chelsea', 'nyc', 'especi', 'lobster', 'caviar', 'also', 'pork', 'ginger', 'come', 'salad', 'appet', 'delici', 'dish', 'good', 'perfect', 'ny', 'bagel', 'asto', 'great', 'place', 'irish', 'breakfast', 'delici', 'mexican', 'food', 'close', 'theatr', 'district', 'slider', 'green', 'salad', 'tasti', 'lemoni', 'notch', 'includ', 'octopu', 'tuna', 'tartar', 'serv', 'breakfast', 'day', 'includ', 'egg', 'bagel', 'smoke', 'fish', 'matzoh', 'bal', 'toni', 'thank', 'god']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# İlk 5 cümleyi yazdıralım\n",
    "for i in range(5):\n",
    " print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")\n",
    " print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")\n",
    " print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d4d95a-3652-49e3-b68a-5b191cfd754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e23051-208d-4777-9555-d30ae919580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelimeleri tokenleştirip, lemmatize etme ve stemleme\n",
    "def preprocess_sentence(sentence):\n",
    " tokens = word_tokenize(sentence) # Cümleyi kelimelere ayır\n",
    "# Sadece harf olan kelimeleri al ve stopword'leri çıkar\n",
    " filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    " lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens] # Lemmatize etme\n",
    " stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens] #Stemleme\n",
    " return lemmatized_tokens, stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38b6101a-cf60-4430-a411-9b957c3094cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her cümleyi tokenleştir, lemmatize et ve stemle\n",
    "tokenized_corpus_lemmatized = []\n",
    "tokenized_corpus_stemmed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f572cc8-4f24-46eb-8a55-7d5975ad6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    " lemmatized_tokens, stemmed_tokens = preprocess_sentence(sentence)\n",
    " tokenized_corpus_lemmatized.append(lemmatized_tokens)\n",
    " tokenized_corpus_stemmed.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9809d94b-fbac-4ba0-a2fb-cab831ce3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec modeli eğitmek için parametreler\n",
    "parameters = [\n",
    "   {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "   {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "   {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "   {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "   {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "   {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "   {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "   {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6824c0b4-cb4f-4b13-a886-6bb4589ebdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonksiyon ile Word2Vec modeli eğitme ve kaydetme\n",
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'], window=params['window'], min_count=1, sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']} model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8031328e-9d56-4247-9d46-76b2dfb13a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100 model saved!\n",
      "lemmatized_model_skipgram_window2_dim100 model saved!\n",
      "lemmatized_model_cbow_window4_dim100 model saved!\n",
      "lemmatized_model_skipgram_window4_dim100 model saved!\n",
      "lemmatized_model_cbow_window2_dim300 model saved!\n",
      "lemmatized_model_skipgram_window2_dim300 model saved!\n",
      "lemmatized_model_cbow_window4_dim300 model saved!\n",
      "lemmatized_model_skipgram_window4_dim300 model saved!\n",
      "stemmed_model_cbow_window2_dim100 model saved!\n",
      "stemmed_model_skipgram_window2_dim100 model saved!\n",
      "stemmed_model_cbow_window4_dim100 model saved!\n",
      "stemmed_model_skipgram_window4_dim100 model saved!\n",
      "stemmed_model_cbow_window2_dim300 model saved!\n",
      "stemmed_model_skipgram_window2_dim300 model saved!\n",
      "stemmed_model_cbow_window4_dim300 model saved!\n",
      "stemmed_model_skipgram_window4_dim300 model saved!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize edilmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "\n",
    "# Stemlenmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7abff8a-25a0-4d5c-8094-a5d70fa785e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dosyalarını yüklemek\n",
    "model_1 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_cbow_window2_dim100.model\")\n",
    "model_2 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_cbow_window4_dim100.model\")\n",
    "model_3 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_cbow_window2_dim300.model\")\n",
    "model_4 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_cbow_window4_dim300.model\")\n",
    "model_5 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_skipgram_window2_dim100.model\")\n",
    "model_6 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_skipgram_window2_dim300.model\")\n",
    "model_7 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_skipgram_window4_dim100.model\")\n",
    "model_8 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/lemmatized_model_skipgram_window4_dim300.model\")\n",
    "model_9 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_cbow_window2_dim100.model\")\n",
    "model_10 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_cbow_window2_dim300.model\")\n",
    "model_11= Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_cbow_window4_dim100.model\")\n",
    "model_12= Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_cbow_window4_dim300.model\")\n",
    "model_13= Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_skipgram_window2_dim100.model\")\n",
    "model_14= Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_skipgram_window2_dim300.model\")\n",
    "model_15 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_skipgram_window4_dim100.model\")\n",
    "model_16 = Word2Vec.load(\"C:/Users/yasin/Desktop/2.hafta/stemmed_model_skipgram_window4_dim300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "334e9a23-f77b-4105-82ba-c90c8896cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'python' kelimesi ile en benzer 3 kelimeyi ve skorlarını yazdırmak\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity = model.wv.most_similar('soup', topn=3)\n",
    "    print(f\"\\n{model_name} Modeli - 'soup' ile En Benzer 3 Kelime:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9719a247-1234-4b3a-865b-c8917aaa47d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lemmatized_model_cbow_window2_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9977431893348694\n",
      "Kelime: fried, Benzerlik Skoru: 0.9969526529312134\n",
      "Kelime: pasta, Benzerlik Skoru: 0.9965759515762329\n",
      "\n",
      "lemmatized_model_cbow_window4_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9983174204826355\n",
      "Kelime: egg, Benzerlik Skoru: 0.9982950687408447\n",
      "Kelime: dish, Benzerlik Skoru: 0.9973147511482239\n",
      "\n",
      "lemmatized_model_cbow_window2_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: authentic, Benzerlik Skoru: 0.9981626272201538\n",
      "Kelime: thin, Benzerlik Skoru: 0.9981129765510559\n",
      "Kelime: nyc, Benzerlik Skoru: 0.9980076551437378\n",
      "\n",
      "lemmatized_model_cbow_window4_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9994454383850098\n",
      "Kelime: nyc, Benzerlik Skoru: 0.9987310767173767\n",
      "Kelime: lobster, Benzerlik Skoru: 0.9984368085861206\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9739777445793152\n",
      "Kelime: french, Benzerlik Skoru: 0.9627479314804077\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9383504390716553\n",
      "\n",
      "lemmatized_model_skipgram_window2_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9807698726654053\n",
      "Kelime: french, Benzerlik Skoru: 0.9722921848297119\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9554046988487244\n",
      "\n",
      "lemmatized_model_skipgram_window4_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9771093726158142\n",
      "Kelime: french, Benzerlik Skoru: 0.9578714370727539\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9248891472816467\n",
      "\n",
      "lemmatized_model_skipgram_window4_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9737265706062317\n",
      "Kelime: french, Benzerlik Skoru: 0.9668052196502686\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9278414249420166\n",
      "\n",
      "stemmed_model_cbow_window2_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9965664744377136\n",
      "Kelime: pasta, Benzerlik Skoru: 0.9962313771247864\n",
      "Kelime: rice, Benzerlik Skoru: 0.9959877729415894\n",
      "\n",
      "stemmed_model_cbow_window2_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9989275932312012\n",
      "Kelime: dish, Benzerlik Skoru: 0.9988440871238708\n",
      "Kelime: pancak, Benzerlik Skoru: 0.9985888600349426\n",
      "\n",
      "stemmed_model_cbow_window4_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9985374808311462\n",
      "Kelime: onion, Benzerlik Skoru: 0.9982149004936218\n",
      "Kelime: pasta, Benzerlik Skoru: 0.9978033900260925\n",
      "\n",
      "stemmed_model_cbow_window4_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: egg, Benzerlik Skoru: 0.9994621872901917\n",
      "Kelime: rice, Benzerlik Skoru: 0.9994065165519714\n",
      "Kelime: pancak, Benzerlik Skoru: 0.9993587136268616\n",
      "\n",
      "stemmed_model_skipgram_window2_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: french, Benzerlik Skoru: 0.9626584053039551\n",
      "Kelime: onion, Benzerlik Skoru: 0.9563021063804626\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9282695651054382\n",
      "\n",
      "stemmed_model_skipgram_window2_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9797003269195557\n",
      "Kelime: french, Benzerlik Skoru: 0.9732134342193604\n",
      "Kelime: chicken, Benzerlik Skoru: 0.953182578086853\n",
      "\n",
      "stemmed_model_skipgram_window4_dim100.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.96946120262146\n",
      "Kelime: french, Benzerlik Skoru: 0.9572476148605347\n",
      "Kelime: toast, Benzerlik Skoru: 0.9231040477752686\n",
      "\n",
      "stemmed_model_skipgram_window4_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: french, Benzerlik Skoru: 0.962284505367279\n",
      "Kelime: onion, Benzerlik Skoru: 0.9586911797523499\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9318863153457642\n"
     ]
    }
   ],
   "source": [
    "print_similar_words(model_1 , \"lemmatized_model_cbow_window2_dim100.model\")\n",
    "print_similar_words(model_2 , \"lemmatized_model_cbow_window4_dim100.model\")\n",
    "print_similar_words(model_3 , \"lemmatized_model_cbow_window2_dim300.model\")\n",
    "print_similar_words(model_4, \"lemmatized_model_cbow_window4_dim300.model\")\n",
    "print_similar_words(model_5, \"lemmatized_model_skipgram_window2_dim100.model\")\n",
    "print_similar_words(model_6, \"lemmatized_model_skipgram_window2_dim300.model\")\n",
    "print_similar_words(model_7 , \"lemmatized_model_skipgram_window4_dim100.model\")\n",
    "print_similar_words(model_8, \"lemmatized_model_skipgram_window4_dim300.model\")\n",
    "print_similar_words(model_9 , \"stemmed_model_cbow_window2_dim100.model\")\n",
    "print_similar_words(model_10, \"stemmed_model_cbow_window2_dim300.model\")\n",
    "print_similar_words(model_11, \"stemmed_model_cbow_window4_dim100.model\")\n",
    "print_similar_words(model_12, \"stemmed_model_cbow_window4_dim300.model\")\n",
    "print_similar_words(model_13, \"stemmed_model_skipgram_window2_dim100.model\")\n",
    "print_similar_words(model_14, \"stemmed_model_skipgram_window2_dim300.model\")\n",
    "print_similar_words(model_15, \"stemmed_model_skipgram_window4_dim100.model\")\n",
    "print_similar_words(model_16, \"stemmed_model_skipgram_window4_dim300.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb2096-3214-4f9e-ac85-a9cec0db5ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
