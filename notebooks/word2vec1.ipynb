{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b41c006-4306-4814-a8c2-5f9bb72a63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a077eb9-e355-4ef1-9137-9d51fd263682",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a116a95-b4db-46fe-a58e-6b573a1581d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"C:/Users/yasin/Desktop/otel-restoran-tahmini/csv/lemmatized_sentences.csv\")\n",
    "df2 = pd.read_csv(\"C:/Users/yasin/Desktop/otel-restoran-tahmini/csv/stemmed_sentences.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5836d294-cfb1-4cc5-a58b-2354dd8dec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns = [\"0\"]\n",
    "\n",
    "# NaN değerleri ve boş stringleri temizle\n",
    "df1 = df1.dropna()\n",
    "df1 = df1[df1[\"0\"].str.strip() != \"\"]\n",
    "\n",
    "df2.columns = [\"0\"]\n",
    "\n",
    "# NaN değerleri ve boş stringleri temizle\n",
    "df2 = df2.dropna()\n",
    "df2 = df2[df2[\"0\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65ca9a8c-3383-4f92-b5ab-c0dc578bca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doğru tokenizasyon fonksiyonu\n",
    "def proper_tokenize(text):\n",
    "    # Özel karakterleri kaldır ve küçük harfe çevir\n",
    "    text = re.sub(r'[^a-zA-ZğüşıöçĞÜŞİÖÇ\\s]', '', text.lower())\n",
    "    # NLTK ile tokenize et\n",
    "    tokens = word_tokenize(text)\n",
    "    # Stopwords'leri ve tek karakterli kelimeleri kaldır\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab688d90-548f-4d38-bc56-99eabfa8cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doğru tokenizasyon uygula\n",
    "df1['tokens'] = df1['0'].apply(proper_tokenize)\n",
    "df2['tokens'] = df2['0'].apply(proper_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18ecdecc-b454-4121-b0df-7cba2ae1cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token listelerini oluştur\n",
    "tokenized_corpus_lemmatized = df1['tokens'].tolist()\n",
    "tokenized_corpus_stemmed = df2['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0885504c-8fd3-4fa5-abe4-1acf79540283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fab5ae91-43c8-4121-af8e-facc162e915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_model(corpus, param, model_prefix):\n",
    "    model_type = param['model_type']\n",
    "    vector_size = param['vector_size']\n",
    "    window = param['window']\n",
    "    \n",
    "    # CBOW (sg=0) veya Skip-gram (sg=1)\n",
    "    sg = 0 if model_type == 'cbow' else 1\n",
    "\n",
    "    model = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=1,\n",
    "        workers=4,\n",
    "        sg=sg\n",
    "    )\n",
    "\n",
    "    model_filename = f\"{model_prefix}_{model_type}_vs{vector_size}_w{window}.model\"\n",
    "    model.save(model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd3caa7d-ca45-43ca-b967-4ef1cb0ce11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as lemmatized_model_cbow_vs100_w2.model\n",
      "Model saved as lemmatized_model_skipgram_vs100_w2.model\n",
      "Model saved as lemmatized_model_cbow_vs100_w4.model\n",
      "Model saved as lemmatized_model_skipgram_vs100_w4.model\n",
      "Model saved as lemmatized_model_cbow_vs300_w2.model\n",
      "Model saved as lemmatized_model_skipgram_vs300_w2.model\n",
      "Model saved as lemmatized_model_cbow_vs300_w4.model\n",
      "Model saved as lemmatized_model_skipgram_vs300_w4.model\n",
      "Model saved as stemmed_model_cbow_vs100_w2.model\n",
      "Model saved as stemmed_model_skipgram_vs100_w2.model\n",
      "Model saved as stemmed_model_cbow_vs100_w4.model\n",
      "Model saved as stemmed_model_skipgram_vs100_w4.model\n",
      "Model saved as stemmed_model_cbow_vs300_w2.model\n",
      "Model saved as stemmed_model_skipgram_vs300_w2.model\n",
      "Model saved as stemmed_model_cbow_vs300_w4.model\n",
      "Model saved as stemmed_model_skipgram_vs300_w4.model\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize edilmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "\n",
    "# Stemlenmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c13ff9e8-2a5f-47cb-9a8a-e340f0323128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model dosyalarını yüklemek\n",
    "model_1 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_cbow_vs100_w2.model\")\n",
    "model_2 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_cbow_vs100_w4.model\")\n",
    "model_3 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_cbow_vs300_w2.model\")\n",
    "model_4 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_cbow_vs300_w4.model\")\n",
    "model_5 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_skipgram_vs100_w2.model\")\n",
    "model_6 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_skipgram_vs100_w4.model\")\n",
    "model_7 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_skipgram_vs300_w2.model\")\n",
    "model_8 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/lemmatized_model_skipgram_vs300_w4.model\")\n",
    "model_9  = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_cbow_vs100_w2.model\")\n",
    "model_10 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_cbow_vs100_w4.model\")\n",
    "model_11 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_cbow_vs300_w2.model\")\n",
    "model_12 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_cbow_vs300_w4.model\")\n",
    "model_13 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_skipgram_vs100_w2.model\")\n",
    "model_14 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_skipgram_vs100_w4.model\")\n",
    "model_15 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_skipgram_vs300_w2.model\")\n",
    "model_16 = Word2Vec.load(\"C:/Users/yasin/Desktop/word2/stemmed_model_skipgram_vs300_w4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b52e78fa-93e2-4686-9744-ec1576fb4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'soup' kelimesi ile en benzer 3 kelimeyi ve skorlarını yazdırmak\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity = model.wv.most_similar(\"soup\", topn=3)\n",
    "    print(f\"\\n{model_name} Modeli - 'soup' ile En Benzer 3 Kelime:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06b834f8-87d8-46c0-baba-48123337eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized CBOW Window 2 Dim 100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: burger, Benzerlik Skoru: 0.9964419007301331\n",
      "Kelime: dish, Benzerlik Skoru: 0.9963705539703369\n",
      "Kelime: chicken, Benzerlik Skoru: 0.996134340763092\n",
      "\n",
      "Stemmed Skipgram Window 4 Dim 100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9988583922386169\n",
      "Kelime: nyc, Benzerlik Skoru: 0.9981061220169067\n",
      "Kelime: cocktail, Benzerlik Skoru: 0.9976577758789062\n",
      "\n",
      "Lemmatized Skipgram Window 2 Dim 300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9991185069084167\n",
      "Kelime: burger, Benzerlik Skoru: 0.9990002512931824\n",
      "Kelime: pasta, Benzerlik Skoru: 0.9987747669219971\n",
      "\n",
      "lemmatized skipgram window 4 dim 100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9995337128639221\n",
      "Kelime: nyc, Benzerlik Skoru: 0.9993067383766174\n",
      "Kelime: cocktail, Benzerlik Skoru: 0.999150812625885\n",
      "\n",
      "lemmatized cbow window 2 dim 300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: french, Benzerlik Skoru: 0.974658191204071\n",
      "Kelime: onion, Benzerlik Skoru: 0.9730984568595886\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9498734474182129\n",
      "\n",
      "lemmatizedskipgramwindow 2 dim300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9816474318504333\n",
      "Kelime: french, Benzerlik Skoru: 0.9627121686935425\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9498215913772583\n",
      "\n",
      "lemmatized_cbow_window 4_dim300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9805237054824829\n",
      "Kelime: french, Benzerlik Skoru: 0.9789203405380249\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9680929780006409\n",
      "\n",
      "lemmatized_skipgram_window4_dim300.model Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9845457673072815\n",
      "Kelime: french, Benzerlik Skoru: 0.9741965532302856\n",
      "Kelime: rice, Benzerlik Skoru: 0.9678078889846802\n",
      "\n",
      "stemmed_cbow_window2_dim100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9971490502357483\n",
      "Kelime: dish, Benzerlik Skoru: 0.9969275593757629\n",
      "Kelime: pasta, Benzerlik Skoru: 0.9966006278991699\n",
      "\n",
      "stemmed_skipgram_window2_dim100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: dish, Benzerlik Skoru: 0.9984486699104309\n",
      "Kelime: onion, Benzerlik Skoru: 0.9983553290367126\n",
      "Kelime: nyc, Benzerlik Skoru: 0.9977248907089233\n",
      "\n",
      "stemmed_cbow_window4_dim100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9990252256393433\n",
      "Kelime: dish, Benzerlik Skoru: 0.9987514019012451\n",
      "Kelime: cocktail, Benzerlik Skoru: 0.9984294176101685\n",
      "\n",
      "stemmed_skipgram_window4_dim100 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: nyc, Benzerlik Skoru: 0.9992662072181702\n",
      "Kelime: dish, Benzerlik Skoru: 0.9992305636405945\n",
      "Kelime: onion, Benzerlik Skoru: 0.9991356134414673\n",
      "\n",
      "stemmed_cbow_window2_dim300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: french, Benzerlik Skoru: 0.9752590656280518\n",
      "Kelime: onion, Benzerlik Skoru: 0.9611130356788635\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9477672576904297\n",
      "\n",
      "stemmed_skipgram_window2_dim300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.9835890531539917\n",
      "Kelime: french, Benzerlik Skoru: 0.9666202068328857\n",
      "Kelime: toast, Benzerlik Skoru: 0.9434374570846558\n",
      "\n",
      "stemmed_cbow_window4_dim300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: french, Benzerlik Skoru: 0.9800978302955627\n",
      "Kelime: onion, Benzerlik Skoru: 0.9791883826255798\n",
      "Kelime: chicken, Benzerlik Skoru: 0.9671450257301331\n",
      "\n",
      "stemmed_skipgram_window4_dim300 Modeli - 'soup' ile En Benzer 3 Kelime:\n",
      "Kelime: onion, Benzerlik Skoru: 0.988004207611084\n",
      "Kelime: french, Benzerlik Skoru: 0.9706580638885498\n",
      "Kelime: noodl, Benzerlik Skoru: 0.9645949006080627\n"
     ]
    }
   ],
   "source": [
    "# 16 model için benzer kelimeleri yazdır\n",
    "print_similar_words(model_1, \"Lemmatized CBOW Window 2 Dim 100\")\n",
    "print_similar_words(model_2, \"Stemmed Skipgram Window 4 Dim 100\")\n",
    "print_similar_words(model_3, \"Lemmatized Skipgram Window 2 Dim 300\")\n",
    "print_similar_words(model_4, \"lemmatized skipgram window 4 dim 100\")\n",
    "print_similar_words(model_5, \"lemmatized cbow window 2 dim 300\")\n",
    "print_similar_words(model_6, \"lemmatizedskipgramwindow 2 dim300\")\n",
    "print_similar_words(model_7, \"lemmatized_cbow_window 4_dim300\")\n",
    "print_similar_words(model_8, \"lemmatized_skipgram_window4_dim300.model\")\n",
    "print_similar_words(model_9, \"stemmed_cbow_window2_dim100\")\n",
    "print_similar_words(model_10, \"stemmed_skipgram_window2_dim100\")\n",
    "print_similar_words(model_11, \"stemmed_cbow_window4_dim100\")\n",
    "print_similar_words(model_12, \"stemmed_skipgram_window4_dim100\")\n",
    "print_similar_words(model_13, \"stemmed_cbow_window2_dim300\")\n",
    "print_similar_words(model_14, \"stemmed_skipgram_window2_dim300\")\n",
    "print_similar_words(model_15, \"stemmed_cbow_window4_dim300\")\n",
    "print_similar_words(model_16, \"stemmed_skipgram_window4_dim300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33f2fb9a-f47b-45b2-b6aa-979ee838d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En sık kullanılan 20 kelime: [('good', 3484), ('staff', 2419), ('room', 2355), ('hotel', 1827), ('great', 1597), ('food', 1578), ('location', 1515), ('nice', 1161), ('clean', 1090), ('service', 992), ('stay', 970), ('available', 759), ('place', 721), ('breakfast', 705), ('best', 703), ('excellent', 694), ('review', 655), ('friendly', 641), ('helpful', 639), ('comment', 635)]\n"
     ]
    }
   ],
   "source": [
    "# Veri setinizde en sık geçen 20 kelime\n",
    "from collections import Counter\n",
    "all_words = [word for sentence in tokenized_corpus_lemmatized for word in sentence]\n",
    "print(\"En sık kullanılan 20 kelime:\", Counter(all_words).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f2357-a380-40cb-96e8-4cfbcf767bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
